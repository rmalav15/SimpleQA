{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "layers = tf.layers\n",
    "rnn = tf.nn.rnn_cell\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"train_processed.csv\"\n",
    "val_file = \"val_processed.csv\"\n",
    "test_file = \"test_processed.csv\"\n",
    "summary_dir = \"version1\"\n",
    "\n",
    "batch_size = 1\n",
    "val_size = 1000\n",
    "max_iter = 50000\n",
    "init_learning_rate = 0.001\n",
    "decay_step = 25000\n",
    "decay_rate = 0.1\n",
    "beta = 0.9\n",
    "\n",
    "lstm_hidden_layer = 100\n",
    "lstm_stack_size = 2\n",
    "question_ans_emb = 150\n",
    "dropout_keep_prob = 0.75\n",
    "num_classes = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.convert_to_tensor([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[6054, 2063, 6206, 4338, 2470,  816, 7011,    0, 1797,   17, 2372,\n",
      "           0, 7011]], dtype=int32), array([[ 147, 3000,    0, 3837]], dtype=int32), array([[5951, 4112, 4398,    0, 2248]], dtype=int32), array([[4984, 4112, 5254,    0, 3958]], dtype=int32), array([[4983,  671, 1226,  579,    0, 5123]], dtype=int32), array([[1]], dtype=int32))\n",
      "(array([[3743, 2569, 5483,    0, 1059, 1658,    0, 6686,    0, 2248, 2434,\n",
      "        3664]], dtype=int32), array([[2434]], dtype=int32), array([[3664]], dtype=int32), array([[2434, 3664]], dtype=int32), array([[2434, 3664]], dtype=int32), array([[2]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "def to_int_tensor(string_tensor):\n",
    "    int_array = tf.string_to_number(tf.string_split([string_tensor]).values,\n",
    "                        out_type=tf.int32)\n",
    "    return tf.convert_to_tensor(int_array)\n",
    "\n",
    "# def train_preprocess(row):\n",
    "#     question = to_int_tensor(row[\"question\"][0])\n",
    "#     Aopt = to_int_tensor(row[\"answerA\"][0])\n",
    "#     Bopt = to_int_tensor(row[\"answerB\"][0])\n",
    "#     Copt = to_int_tensor(row[\"answerC\"][0])\n",
    "#     Dopt = to_int_tensor(row[\"answerD\"][0])\n",
    "#     label = row[\"correctAnswer\"]\n",
    "#     return question, Aopt, Bopt, Copt, Dopt, label\n",
    "\n",
    "def preprocess(id, Aopt, Bopt, Copt, Dopt, label, question, text):\n",
    "    question = to_int_tensor(question)\n",
    "    Aopt = to_int_tensor(Aopt)\n",
    "    Bopt = to_int_tensor(Bopt)\n",
    "    Copt = to_int_tensor(Copt)\n",
    "    Dopt = to_int_tensor(Dopt)\n",
    "    return question, Aopt, Bopt, Copt, Dopt, [label]\n",
    "\n",
    "\n",
    "# id,answerA,answerB,answerC,answerD,correctAnswer,question,text\n",
    "type_defaults = [tf.string, tf.string, tf.string, tf.string,  tf.string, tf.int32, tf.string, tf.string,]\n",
    "\n",
    "train_dataset = tf.data.experimental.CsvDataset(train_file, record_defaults=type_defaults,\n",
    "                                             header=True)\n",
    "train_dataset = train_dataset.map(preprocess).batch(batch_size).shuffle(buffer_size=10000).repeat()\n",
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "train_batch = train_iterator.get_next()\n",
    "\n",
    "\n",
    "val_dataset = tf.data.experimental.CsvDataset(val_file, record_defaults=type_defaults,\n",
    "                                             header=True)\n",
    "val_dataset = val_dataset.map(preprocess).batch(batch_size).repeat()\n",
    "val_iterator = val_dataset.make_one_shot_iterator()\n",
    "val_batch = val_iterator.get_next()\n",
    "\n",
    "#sess.run((train_iterator.initializer, val_iterator.initializer))\n",
    "print(sess.run(train_batch))\n",
    "print(sess.run(val_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_qeustion_answer_model(ques_emb, ans_emb, reuse=False):\n",
    "    with tf.variable_scope(\"single_qeustion_answer_model\", reuse=reuse):\n",
    "        with tf.variable_scope(\"question_lstm\"):\n",
    "            forward_stack = [rnn.DropoutWrapper(\n",
    "                rnn.GRUCell(lstm_hidden_layer),\n",
    "                input_keep_prob=dropout_keep_prob,\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "                for _ in range(lstm_stack_size)]\n",
    "            forward_cell = rnn.MultiRNNCell(forward_stack, state_is_tuple=False)\n",
    "\n",
    "            backward_stack = [rnn.DropoutWrapper(\n",
    "                rnn.GRUCell(lstm_hidden_layer),\n",
    "                input_keep_prob=dropout_keep_prob,\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "                for _ in range(lstm_stack_size)]\n",
    "            backward_cell = rnn.MultiRNNCell(backward_stack, state_is_tuple=False)\n",
    "\n",
    "            (ques_fw, ques_bw), _ = tf.nn.bidirectional_dynamic_rnn(forward_cell,\n",
    "                                                                    backward_cell,\n",
    "                                                                    ques_emb,\n",
    "                                                                    dtype=tf.float32)\n",
    "            ques_output = tf.concat([ques_fw[-1], ques_bw[-1]], axis=1)\n",
    "\n",
    "        with tf.variable_scope(\"answer_lstm\"):\n",
    "            forward_stack = [rnn.DropoutWrapper(\n",
    "                rnn.GRUCell(lstm_hidden_layer),\n",
    "                input_keep_prob=dropout_keep_prob,\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "                for _ in range(lstm_stack_size)]\n",
    "            forward_cell = rnn.MultiRNNCell(forward_stack, state_is_tuple=False)\n",
    "\n",
    "            backward_stack = [rnn.DropoutWrapper(\n",
    "                rnn.GRUCell(lstm_hidden_layer),\n",
    "                input_keep_prob=dropout_keep_prob,\n",
    "                output_keep_prob=dropout_keep_prob)\n",
    "                for _ in range(lstm_stack_size)]\n",
    "            backward_cell = rnn.MultiRNNCell(backward_stack, state_is_tuple=False)\n",
    "\n",
    "            (ans_fw, ans_bw), _ = tf.nn.bidirectional_dynamic_rnn(forward_cell,\n",
    "                                                                  backward_cell,\n",
    "                                                                  ans_emb,\n",
    "                                                                  dtype=tf.float32)\n",
    "            ans_output = tf.concat([ans_fw[-1], ans_bw[-1]], axis=1)\n",
    "\n",
    "        ques_ans_concat = tf.concat([ques_output, ans_output], axis=1)\n",
    "\n",
    "        output = layers.dense(ques_ans_concat, question_ans_emb, activation=tf.nn.relu)\n",
    "        return layers.dropout(output, rate=1 - dropout_keep_prob)\n",
    "\n",
    "\n",
    "def model(question, Aopt, Bopt, Copt, Dopt, label, embmat, reuse=False):\n",
    "    Network = collections.namedtuple('Network', 'loss, pred, \\\n",
    "                                        grads_and_vars, \\\n",
    "                                        train, global_step, learning_rate')\n",
    "    with tf.variable_scope(\"BiRNN\", reuse=reuse):\n",
    "        question_emb = tf.nn.embedding_lookup(embmat, question, name=\"que_lookup\")\n",
    "    Aopt_emb = tf.nn.embedding_lookup(embmat, Aopt, name=\"a_lookup\")\n",
    "    Bopt_emb = tf.nn.embedding_lookup(embmat, Bopt, name=\"blookup\")\n",
    "    Copt_emb = tf.nn.embedding_lookup(embmat, Copt, name=\"clookup\")\n",
    "    Dopt_emb = tf.nn.embedding_lookup(embmat, Dopt, name=\"dlookup\")\n",
    "\n",
    "    question_AA = single_qeustion_answer_model(question_emb, Aopt_emb)\n",
    "    question_BB = single_qeustion_answer_model(question_emb, Bopt_emb, reuse=True)\n",
    "    question_CC = single_qeustion_answer_model(question_emb, Copt_emb, reuse=True)\n",
    "    question_DD = single_qeustion_answer_model(question_emb, Dopt_emb, reuse=True)\n",
    "\n",
    "    all_emb = tf.concat([question_AA, question_BB, question_CC, question_DD], axis=1)\n",
    "    logits = layers.dense(all_emb, num_classes)\n",
    "\n",
    "    pred_probs = tf.nn.softmax(logits)\n",
    "    pred_label = tf.argmax(pred_probs, axis=1)\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "\n",
    "    total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label,\n",
    "                                                                  logits=logits))\n",
    "\n",
    "    with tf.variable_scope(\"global_step_and_learning_rate\", reuse=reuse):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(init_learning_rate,\n",
    "                                                   global_step,\n",
    "                                                   decay_step,\n",
    "                                                   decay_rate,\n",
    "                                                   staircase=True)\n",
    "        incr_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "    with tf.variable_scope(\"optimizer\", reuse=reuse):\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='BiRNN')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, beta1=beta)\n",
    "            grads_and_vars = optimizer.compute_gradients(total_loss, tvars)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    return Network(\n",
    "        loss=total_loss,\n",
    "        pred=pred_label,\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        train=tf.group(total_loss, incr_global_step, train_op),\n",
    "        global_step=global_step,\n",
    "        learning_rate=learning_rate\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
